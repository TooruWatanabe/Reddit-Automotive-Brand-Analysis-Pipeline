import os
import re
import json
import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict

import pandas as pd

import matplotlib
matplotlib.use("Agg")

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from dotenv import load_dotenv
import praw

# ==== CrewAI ====
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool

# =========================
# 0. ç’°å¢ƒæº–å‚™
# =========================
load_dotenv()
Path("data").mkdir(exist_ok=True)
Path("images").mkdir(exist_ok=True)

# NLTK è¾æ›¸
try:
    nltk.data.find("sentiment/vader_lexicon.zip")
except LookupError:
    nltk.download("vader_lexicon")

# Reddit èªè¨¼
reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    user_agent=os.getenv("REDDIT_USER_AGENT"),
)

# è§£æå¯¾è±¡
TARGET_SUBS = ["cars", "whatcarshouldIbuy", "askcarsales"]
LIMIT_PER_SUB = 200

# ãƒ¡ãƒ¼ã‚«ãƒ¼è¾æ›¸
BRANDS: Dict[str, List[str]] = {
    "Toyota": ["toyota"],
    "Tesla": ["tesla"],
    "BMW": ["bmw"],
    "Honda": ["honda"],
    "Ford": ["ford"],
    "Mercedes": ["mercedes", "mercedes-benz", "benz"],
    "Volkswagen": ["volkswagen", "vw"],
    "Nissan": ["nissan"],
    "Hyundai": ["hyundai"],
    "Kia": ["kia"],
    "Chevrolet": ["chevrolet", "chevy"],
    "Volvo": ["volvo"],
    "GM": ["gm", "general motors"],
    "Stellantis": ["stellantis"],
    "Rivian": ["rivian"],
    "Chrysler": ["chrysler"],
    "Jeep": ["jeep"],
    "Dodge": ["dodge"],
    "Ram": ["ram"],
}

CUSTOM_STOPWORDS = {
    "car", "cars",
    "t", "ve", "re", "ll", "im", "u",
    "dont", "doesnt", "cant", "didnt", "isnt", "wont",
}

# =========================
# 1. ãƒ„ãƒ¼ãƒ«å†…éƒ¨é–¢æ•°
# =========================
def detect_brand(text: str) -> str | None:
    t = text.lower()
    for brand, kws in BRANDS.items():
        for kw in kws:
            if kw in t:
                return brand
    return None

def collect_posts() -> pd.DataFrame:
    rows = []
    for sub in TARGET_SUBS:
        for post in reddit.subreddit(sub).hot(limit=LIMIT_PER_SUB):
            title = post.title or ""
            body = post.selftext or ""
            brand = detect_brand(title + " " + body)
            rows.append({
                "subreddit": sub,
                "title": title,
                "selftext": body,
                "brand": brand,
                "score": getattr(post, "score", None),
                "upvote_ratio": getattr(post, "upvote_ratio", None),
                "created": datetime.datetime.fromtimestamp(post.created_utc),
                "url": post.url
            })
    df = pd.DataFrame(rows)
    df.to_csv("data/raw_posts.csv", index=False)
    return df

def brand_counts(df: pd.DataFrame) -> pd.DataFrame:
    counted = df.dropna(subset=["brand"])["brand"].value_counts().rename_axis("brand").reset_index(name="count")
    counted.to_csv("data/brand_counts.csv", index=False)
    return counted

def brand_texts(df: pd.DataFrame, brand: str) -> tuple[str, set]:
    texts = df.loc[df["brand"] == brand, ["title", "selftext"]].fillna("").agg(" ".join, axis=1).tolist()
    corpus = " ".join(texts)
    corpus = re.sub(r"\b[a-zA-Z]\b", " ", corpus)
    sw = set(STOPWORDS).union(CUSTOM_STOPWORDS, {brand.lower(), *BRANDS[brand]})
    return corpus, sw

def make_wordcloud(corpus: str, stopwords: set[str], brand: str) -> str:
    if not corpus.strip():
        return ""
    wc = WordCloud(
        width=1200, height=800,
        background_color="white",
        stopwords=stopwords,
        collocations=False
    ).generate(corpus)
    out = f"images/{brand.lower()}_wordcloud.png"
    plt.figure(figsize=(12,8))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"{brand} WordCloud (Cleaned)", fontsize=20)
    plt.tight_layout()
    plt.savefig(out)
    plt.close()
    return out

def sentiments(df: pd.DataFrame, brand: str) -> dict:
    texts = df.loc[df["brand"] == brand, ["title", "selftext"]].fillna("").agg(" ".join, axis=1).tolist()
    if not texts:
        return {"pos":0,"neg":0,"neu":0,"n":0}
    analyzer = SentimentIntensityAnalyzer()
    pos = neg = neu = 0
    for t in texts:
        s = analyzer.polarity_scores(t)
        if s["compound"] > 0.05:
            pos += 1
        elif s["compound"] < -0.05:
            neg += 1
        else:
            neu += 1
    res = {"pos":pos, "neg":neg, "neu":neu, "n":len(texts)}
    with open(f"data/{brand.lower()}_sentiment.json", "w") as f:
        json.dump(res, f, ensure_ascii=False, indent=2)
    return res

def summarize_findings(counts: pd.DataFrame, senti_map: dict, notes_map: dict) -> str:
    md = []
    md.append("## ç·æ‹¬ï¼ˆãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰\n")
    top3 = counts.head(3).to_dict("records")
    hl = ", ".join([f"{r['brand']}({r['count']})" for r in top3])
    md.append(f"- è¨€åŠä»¶æ•°ãƒˆãƒƒãƒ—: {hl}\n")
    md.append("- æ„Ÿæƒ…ã¯ brandã”ã¨ã«ãƒã‚¸/ãƒã‚¬/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‚’é›†è¨ˆã—ã€æŠ•ç¨¿æœ¬æ–‡ï¼‹ã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã§VADERåˆ¤å®šã€‚\n")

    md.append("\n## ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥ æ‰€è¦‹\n")
    for brand in counts["brand"].tolist():
        s = senti_map.get(brand, {"pos":0, "neg":0, "neu":0, "n":0})
        note = notes_map.get(brand, "")
        posr = f"{round(100*s['pos']/s['n'],1)}%" if s["n"] else "-"
        negr = f"{round(100*s['neg']/s['n'],1)}%" if s["n"] else "-"
        neur = f"{round(100*s['neu']/s['n'],1)}%" if s["n"] else "-"
        md.append(f"### {brand}\n- è¨€åŠä»¶æ•°: {counts.loc[counts['brand']==brand, 'count'].item()}\n"
                  f"- æ„Ÿæƒ…: ãƒã‚¸ {s['pos']} ({posr}) / ãƒã‚¬ {s['neg']} ({negr}) / ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ« {s['neu']} ({neur})\n"
                  f"- ãƒ¡ãƒ¢: {note}\n")
    return "\n".join(md)

# =========================
# 2. ãƒ„ãƒ¼ãƒ«ã‚¯ãƒ©ã‚¹
# =========================

class CollectTool(BaseTool):
    name: str = "Redditåé›†ãƒ„ãƒ¼ãƒ«"
    description: str = "Redditã‹ã‚‰æŠ•ç¨¿ã‚’åé›†ã—CSVã«ä¿å­˜ã™ã‚‹"

    def _run(self, **kwargs) -> str:
        df = collect_posts()

        # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ç”¨æ„
        outpath = Path("data/raw_posts.csv")
        outpath.parent.mkdir(exist_ok=True)

        # CSVã«ä¿å­˜
        df.to_csv(outpath, index=False, encoding="utf-8")

        return f"åé›†å®Œäº†: {len(df)}ä»¶ ({outpath}) ã‚’ç”Ÿæˆã—ã¾ã—ãŸ"



class AnalysisTool(BaseTool):
    name: str = "Redditåˆ†æãƒ„ãƒ¼ãƒ«"
    description: str = "åé›†ã—ãŸæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒ–ãƒ©ãƒ³ãƒ‰ã”ã¨ã®ä»¶æ•°ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€æ„Ÿæƒ…åˆ†æã‚’è¡Œã†"

    def _run(self, raw_posts_file: str = "data/raw_posts.csv", **kwargs) -> str:
        input_path = Path(raw_posts_file)
        if not input_path.exists():
            return f"âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}"

        df = pd.read_csv(input_path)

        # ãƒ–ãƒ©ãƒ³ãƒ‰é›†è¨ˆ
        counts = df["brand"].value_counts().reset_index()
        counts.columns = ["brand", "count"]
        counts.to_csv("data/brand_counts.csv", index=False, encoding="utf-8")

        # æ„Ÿæƒ…é›†è¨ˆï¼ˆä¾‹ï¼‰
        sentiments = {"pos": 0, "neg": 0, "neu": 0}
        for text in df["text"].tolist():
            s = simple_sentiment(text)
            sentiments[s] += 1
        sentiments["n"] = len(df)
        with open("data/sentiments_all.json", "w", encoding="utf-8") as f:
            json.dump(sentiments, f, ensure_ascii=False, indent=2)

        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
        for brand in counts["brand"].tolist():
            texts = df[df["brand"] == brand]["text"].tolist()
            if not texts:
                continue
            text_blob = " ".join(texts)
            wc = WordCloud(width=800, height=400, background_color="white").generate(text_blob)
            wc.to_file(f"images/{brand.lower()}_wordcloud.png")

        return "åˆ†æå®Œäº†: brand_counts.csv, sentiments_all.json, ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸ"



class ReportTool(BaseTool):
    name: str = "ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«"
    description: str = "Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹"

    def _run(self, **kwargs) -> str:
        brand_counts_path = Path(kwargs.get("brand_counts", "data/brand_counts.csv"))
        sentiment_path = Path(kwargs.get("sentiment", "data/sentiments_all.json"))
        wc_dir = Path(kwargs.get("word_cloud_dir", "images"))

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        counts = pd.read_csv(brand_counts_path)
        with open(sentiment_path, encoding="utf-8") as f:
            senti_map = json.load(f)

        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒãƒ‘ã‚¹
        wc_map = {
            b: str(wc_dir / f"{b.lower()}_wordcloud.png")
            for b in counts["brand"].tolist()
            if (wc_dir / f"{b.lower()}_wordcloud.png").exists()
        }

        today = datetime.date.today()
        md_path = Path(f"data/reddit_auto_brand_report_{today}.md")

        with md_path.open("w", encoding="utf-8") as f:
            f.write(f"# ğŸš— Reddit è‡ªå‹•è»Šãƒ–ãƒ©ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"- åé›†æ—¥: {today}\n\n")

            # ãƒ–ãƒ©ãƒ³ãƒ‰ä»¶æ•°
            f.write("## ãƒ–ãƒ©ãƒ³ãƒ‰è¨€åŠä»¶æ•°\n\n")
            f.write(counts.to_markdown(index=False))
            f.write("\n\n")

            # æ„Ÿæƒ…åˆ†æ
            f.write("## æ„Ÿæƒ…åˆ†æçµæœ\n\n")
            f.write("| ãƒ–ãƒ©ãƒ³ãƒ‰ | ãƒã‚¸ | ãƒã‚¬ | ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ« | ä»¶æ•° |\n|---|---:|---:|---:|---:|\n")
            for brand in counts["brand"].tolist():
                s = senti_map.get(brand, {"pos": 0, "neg": 0, "neu": 0, "n": 0})
                f.write(f"| {brand} | {s['pos']} | {s['neg']} | {s['neu']} | {s['n']} |\n")
            f.write("\n\n")

            # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
            f.write("## ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰\n\n")
            for brand, path in wc_map.items():
                f.write(f"### {brand}\n![{brand}]({path})\n\n")

        return str(md_path)



# =========================
# 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã‚¿ã‚¹ã‚¯
# =========================
collector = Agent(role="åé›†", goal="RedditæŠ•ç¨¿ã‚’é›†ã‚ã‚‹", backstory="å…¬å¹³ãªãƒ‡ãƒ¼ã‚¿åé›†æ‹…å½“")
analyzer = Agent(role="åˆ†æ", goal="æŠ•ç¨¿ã‚’åˆ†æã™ã‚‹", backstory="ãƒ‡ãƒ¼ã‚¿è§£ææ‹…å½“")
reporter = Agent(role="ãƒ¬ãƒãƒ¼ãƒˆ", goal="Markdownã‚’æ›¸ã", backstory="è€ƒå¯Ÿã‚’ã¾ã¨ã‚ã‚‹æ‹…å½“")

task1 = Task(
    description="RedditæŠ•ç¨¿ã‚’åé›†",
    agent=collector,
    tools=[CollectTool()],
    expected_output="raw_posts.csv ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨"
)

task2 = Task(
    description="ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥åˆ†æ",
    agent=analyzer,
    tools=[AnalysisTool()],
    expected_output="brand_counts.csv ã‚„ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€sentiment.json ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨"
)

task3 = Task(
    description="Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ",
    agent=reporter,
    tools=[ReportTool()],   # âœ… BaseTool ç¶™æ‰¿æ¸ˆã¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    expected_output="ãƒ–ãƒ©ãƒ³ãƒ‰åˆ†æã«åŸºã¥ã„ãŸMarkdownãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«"
)



# =========================
# 4. å®Ÿè¡Œ
# =========================
def run():
    crew = Crew(agents=[collector, analyzer, reporter], tasks=[task1, task2, task3], process=Process.sequential, verbose=True)
    result = crew.kickoff()
    print("\nâœ… å®Œäº†:", result)

if __name__ == "__main__":
    run()

